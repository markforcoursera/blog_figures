---
title: "JHUML Final Project Submission"
author: "markb"
date: "September 4, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Summary
The overall goal of this small project was to produce a prediction model to determine the quality of a performed exercise based on data from accelerometers. A final prediction model was produced by using a random forest based model. This final model was found to be reasonably accurate, with a final estimated (out-of-sample) accuracy of 93.9%.

##Introduction
The goal of this project was to create a prediction model for determining the quality of exercise done by individuals based on accelerometers. A full description of this dataset and associated publications are available, as described in the course materials.

##Results and Methods
In this section, a step-by-step description of the production and testing of the model is given.

###Setup: load libraries and training data

Here, we also show a summary of all columns to indicate ones that we will end up eliminating.

```{r loaddata_and_libraries}
library(caret)
library(e1071)
wltrain.full <- read.csv("C:\\Rstuff\\finalprojdata\\pml-training.csv")
#look at data
dim(wltrain.full)
table(wltrain.full$classe)
summary(wltrain.full)
```

##FIRST LOOK CONCLUSIONS ON DATASET
There were reasonable number at each level in classe, so we don't have to worry about some levels of classe being poorly represented in samples.
The data features many variables that have large numbers of missing values (NA) or values that are not numbers, such as apparent division by zero errors.

The modeling strategy will begin by deleting these columns and producing a model using the remaining columns. Also, the first loaded column (which just corresponds to a row number) and the time columns will be deleted to avoid dependence on time values. 

In addition, columns concerning the window number will be deleted, as these may also be direct indices to the final classe variable.



##INITIAL DATA EXPLORATION
Initial data exploration demonstrated that there was a large amount of cases and only 5 levels of "classe". So this implies that a smaller subset should be adequate. Use of a small subset in this case is particularly appropriate, given limitations in computer power.

Initial training will be done with 10% of the data. A further internal testing set was created with the remaining 90% of the full dataset. All were created using the createDatapartition function from the caret library to allow a reasonable sampling. 

To determine if the 10% is adequate, the performance on the remaining dataset was examined. If the performance on these addition sets of data is poor, then this could be due to poor modeling or a sampling issue.

##CREATING THE 10% dataset


```{r makesmallset}
#try 10% to begin
set.seed(505)
tenperdex <- createDataPartition(wltrain.full$classe,p=0.1,list=FALSE)
wl.small <- wltrain.full[tenperdex, ]
```


##Eliminating columns that have NAs or bad values or are linked directly to the final groups


All of these columns may cause problems. I chose to eliminate many columns that seemed like they would have correlations with the final classe variable solely due to things like the observations being in a certain order. Inspection of columns using the summary() function led to the deletion of several columns.

```{r fixcols}
wl.small.nona <- wl.small[sapply(wl.small, function(x) !any(is.na(x)))]

#now, kill more that have bad values in them
badklist <- grep("kurtosis",colnames(wl.small.nona))
badslist <- grep("skewness",colnames(wl.small.nona))
badyawlist <- grep("_yaw_",colnames(wl.small.nona))
totbad <- c(badklist,badslist,badyawlist)

wl.small.v2 <- wl.small.nona[ , -totbad]
#kill the time columns
wl.small.v3 <- wl.small.v2[ , -c(3,4,5)] #eliminate some time columns
wl.small.v4 <- wl.small.v3[ , -c(1)] #eliminate row index column
wl.small.v5 <- wl.small.v4[ , -c(2,3)] #eliminate some final ones that seem to be falsely correlated
```

##Make the testing group

```{r makegroup}
wl.remainder <- wltrain.full[-tenperdex, ]
```

##Make RandomForest model and performing predictions on the remaining 90% test set.
For random forest, we  used cross-validation with 4 sets. We choose random forest because this modeling approach was repeatedly mentioned to give good results in lectures, based on performance in competitions. We also tried linear discriminant analysis (data not shown) but models performed relatively poorly with high misclassification rates.

Note that the last lines of code indicate the predictions for the test set and the misclassification level.

```{r rfmodel}

wl.small.v5.rf3 <- train(classe ~ ., method="rf", data=wl.small.v5,trControl=trainControl(method="cv",number=4), prox=TRUE, allowParallel=TRUE)
rfpred <- predict(wl.small.v5.rf3,wl.remainder)
confusionMatrix(rfpred,wl.remainder$classe)
misClassified <- function(vals, preds){
    sum(preds != vals)/length(vals)}
misClassified(wl.remainder$classe,rfpred)

```

##FINAL CONCLUSIONS
The accuracy of 93.9% implies a reasonable out-of-sample error rate, as measured by misclassification (6.1%). This misclassification rate is somewhat troubling and additional studies seem warranted.


